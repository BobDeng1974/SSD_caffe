# 7-9



 1. 对模型进行分析比对, 建立新模型

 2. 多尺度融合, ssd框架

 3. 模型分析:

    程序: 自动保存小于thre 的图片, 并且在同一个分析excel 中自动添加分数, 网络名称

    程序:  自动建立新模型, 改名, 改地址

 4. ssd 数据层, 

    程序: 数据层测试, 可视化

 5. faster rcnn论文

 6. r-fcn 论文




## sdd BBoxData 数据层

```protobuf
name: "DetNet_DarkNet20180519FromPose_TrunkBD_PDHeadHand_DataMinS0.75MaxS2.0NoExp_HisiDataOnlySmpBody_WD5e-3_1A_train"
layer {
  name: "data"
  type: "BBoxData"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  unified_data_transform_param {
    emit_coverage_thre: 0.25
    flip_prob: 0.5
    resized_width: 512
    resized_height: 288
    visualize: false
    save_dir: "/home/zhangming/Datasets/vis_aug_dj"
    dis_param {
      brightness_prob: 0.2
      brightness_delta: 20
      contrast_prob: 0.2
      contrast_lower: 0.5
      contrast_upper: 1.5
      hue_prob: 0.2
      hue_delta: 18
      saturation_prob: 0.2
      saturation_lower: 0.5
      saturation_upper: 1.5
      random_order_prob: 0
    }
    batch_sampler {
      sampler {
        min_scale: 0.75
        max_scale: 1.0
        min_aspect_ratio: 0.5
        max_aspect_ratio: 2.0
      }
      sample_constraint {
        min_object_coverage: 0.9
      }
      max_sample: 1
      max_trials: 50
    }
    emit_area_check: 0.02
    emit_area_check: 0.1
    emit_area_check: 0.3
    emit_area_check: 1.0
    emit_coverage_thre_multiple: 1.0
    emit_coverage_thre_multiple: 0.75
    emit_coverage_thre_multiple: 0.5
    emit_coverage_thre_multiple: 0.25
    sample_sixteennine: true		# 16:9 参数设定
    for_body: true
  }
  unified_data_param {
    shuffle: true
    rand_skip: 500
    batch_size: 4
    mean_value: 104
    mean_value: 117
    mean_value: 123
    add_parts: true
    
    xml_list_multiple: "/home/xjx/xjx_old/work/data/VOCdevkit/VOC2007/ImageSets/Layout/train.txt"
    xml_root_multiple: "/home/zhangming/Datasets/RemoCoco"
 
  }
}
```



    ```protobuf

message UnifiedTransformationParameter {
  optional bool sample_sixteennine = 14 [default = true];
  optional float emit_coverage_thre = 1 [default = 0.25];   # 发出覆盖的阈值
  repeated float emit_coverage_thre_multiple = 13;  
  repeated float emit_area_check = 12;  					# 区域面积检测
  optional int32 kps_min_visible = 2 [default = 4];			# kps 最小可见
  optional float flip_prob = 3 [default = 0.5];				#
  optional int32 resized_width = 4 [default = 512];
  optional int32 resized_height = 5 [default = 288];
  optional bool visualize = 6 [default = false];
  optional string save_dir = 7 [default = ""];
  optional DistortionParameter dis_param = 10;
  repeated BatchSampler batch_sampler = 11;		#			批量采样器
}
    ```

label (1,1,1,9), data (1,3,w,h)

```c++
static_cast<caffe::rng_t*>(prefetch_rng_->generator());
Batch<Dtype>* batch
vector<cv::Mat> 
vector<vector<BBoxData<Dtype> > > 
read xml文件 
void BBoxDataAddPoseLayer<Dtype>::ReadAnnoDataFromXml(const int bindex, const string& xml_file, const string& root_dir,AnnoData<Dtype>* anno) 
reset
BoundingBox<Dtype>::BoundingBox(): scale_factor_(kScaleFactor) {
}  // 构造函数的初始化列表　，scale_factor_ = kScaleFactror
```



为什么要skip 数据呢? 

### 

### bbox_data_layer.cpp

`数据层 常常包括  `1.  DataLayerSetUp,   2. ShuffleLists    3. load_batch

`DataLayerSetUp`

//进行均值处理

导入xml 文件路径,  存到lines_ 中

随机乱序(shuffle)

随机跳过(rand_skip)

// data 处理

height , width 是resize 的尺寸

top[0]->Reshape(batch_size, 3, height, width) 

//label

top[1]->Reshape(1,1,1,9);



`load_batch`

batch  ,  batch_size(), height , width , image_all,  bboxes_all , lines_size , num_p 

item_id (每个batch_size), 

for item_id < batch_size, 

​	读取xml 成anno,   ReadAnnoDataFromXml

​	//转换

​	bboxes , crop_bbox,  doflip,  num_person 

​	两种 Transform ,   包含bboxes, crop_bbox , 不包含

batch->data_.Reshape(batch_size,3,height,width)

normlize , offset= height*width, 

for i < batch_size , 

​	top_data_item

​	for map(w, h ) , 遍历整张图

​		均值处理, 

//top[1]

num_gt, count,  idx 

top = (1,1,num_gt, 9)     bindex, cid, pid,  is_diff,  is_crowd,  x1, y1, x2, y2



### bbox_multi_resize_layer.cpp

取均值使用width, height 的参数而不是原图尺寸

### bbox_data_transformer.cpp

#### randomExpand

expand_bbox,   expand_ratio, height 为 img_height*expand_ratio, 

expand_ratio, expand_prob  , prob 

height , width 是扩展之后的尺寸

### randomCrop

两种 `randomCrop`, 一个是包括crop box, 一个不包括,  

### bbox_data_transformer_multi_resize.cpp

getCropBBox 不同

prob, 





### AnnoData

AnnoData:  img_path ,dataset, img_width , img_height , num_person, instances

instances :  bindex , cid , pid , is_diff , iscrowd , mask_included, mask_path, kps_included, num_kps, joint, BoundingBox 

joint :  关键点描述信息,   joints ,  isVisible

BoundingBox:  x1, x2, x3, x4, scale_factor #比例因子 　通过构造函数 得到4个坐标点 x1, x2,x3, x4

BBoxData : bindex  cid  pid  is_diff  iscrowd   BoundingBox  bbox



### getCropBBox

init : h1 = 90,  w1 = 160,     h2 = 160, w2 = 90

1:  h-max = 90 /90 = 1 ,  w_max = 90 *16/9 = 160 /160 = 1 

prob , 



### reset

```c++
static void reset(MEX_ARGS) { 
    //   MEX_ARGS   int nlhs, mxArray **plhs, int nrhs, const mxArray **prhs

  mxCHECK(nrhs == 0, "Usage: caffe_('reset')");

  // Clear solvers and stand-alone nets

  mexPrintf("Cleared %d solvers and %d stand-alone nets\n",

      solvers.size(), nets.size());

  solvers_.clear();

  nets_.clear();

  // Generate new init_key, so that handles created before becomes invalid

  init_key = static_cast<double>(caffe_rng_rand());

}

```





## 构建新模型

多尺度, 问题什么时候用relu

```protobuf
layer {
  name: "convf"
  type: "Eltwise"
  bottom: "conv4_5_adap"
  bottom: "conv5_5_Upsample"
  top: "convf"
}

// 两者的区别"????/
layer {
  name: "featuremap3"
  type: "Eltwise"
  bottom: "conv5_5_adapfeat3"
  bottom: "conv6_5_adapfeat3"
  top: "featuremap3"
  eltwise_param {
    operation: SUM
  }
}
```

问题:  卷及输出 shape 不对

查看 pad 和kernel_size 的关系   wo = (wi + 2*pad - kernel_size)/s + 1

| pad  | kernel_size |
| ---- | ----------- |
| 0    | 1           |
| 1    | 3           |
| 2    | 5           |



## solver.prototxt 文件



# 7-11 

1. 写新框架的测试程序
2.  测试data 层,  
3.  看loss , mAP, AP, 参数



写个视频测试, 文件夹 所有视频进行测试保存,   

- 保存1 . 原视频画框输出,  保存2.  不符合的图片输出 (flag, 输出不符合的原图片)

图片测试文件   

- 1. 原所有图片画框输出,   2.图片不符合输出  (flag, 输出不符合的原图片)

摄像头测试文件

- 1. flag(保存摄像头原图像,),  flag 保存摄像头画框图像, 



二次分类 进展

多尺度的进展

新模型的进展

## proto

``` protobuf
message UnifiedTransformationParameter {
  optional bool sample_sixteennine = 14 [default = true];
  optional float emit_coverage_thre = 1 [default = 0.25];
  repeated float emit_coverage_thre_multiple = 13;
  repeated float emit_area_check = 12;
  optional int32 kps_min_visible = 2 [default = 4];
  optional float flip_prob = 3 [default = 0.5];
  optional int32 resized_width = 4 [default = 512];
  optional int32 resized_height = 5 [default = 288];
  optional bool visualize = 6 [default = false];
  optional string save_dir = 7 [default = ""];
  optional DistortionParameter dis_param = 10;
  repeated BatchSampler batch_sampler = 11;
}
```

```protobuf
message DistortionParameter {
  // The probability of adjusting brightness.
  optional float brightness_prob = 1 [default = 0.0];
  // Amount to add to the pixel values within [-delta, delta].
  // The possible value is within [0, 255]. Recommend 32.
  optional float brightness_delta = 2 [default = 0.0];

  // The probability of adjusting contrast.
  optional float contrast_prob = 3 [default = 0.0];
  // Lower bound for random contrast factor. Recommend 0.5.
  optional float contrast_lower = 4 [default = 0.0];
  // Upper bound for random contrast factor. Recommend 1.5.
  optional float contrast_upper = 5 [default = 0.0];

  // The probability of adjusting hue.
  optional float hue_prob = 6 [default = 0.0];
  // Amount to add to the hue channel within [-delta, delta].
  // The possible value is within [0, 180]. Recommend 36.
  optional float hue_delta = 7 [default = 0.0];

  // The probability of adjusting saturation.
  optional float saturation_prob = 8 [default = 0.0];
  // Lower bound for the random saturation factor. Recommend 0.5.
  optional float saturation_lower = 9 [default = 0.0];
  // Upper bound for the random saturation factor. Recommend 1.5.
  optional float saturation_upper = 10 [default = 0.0];

  // The probability of randomly order the image channels.
  optional float random_order_prob = 11 [default = 0.0];
}
```

```protobuf
message UnifiedDataParameter {
  optional string xml_list = 1;
  optional string xml_root = 2;
  optional bool shuffle = 3 [default = true];
  optional int32 rand_skip = 4 [default = 100];
  optional int32 batch_size = 5 [default = 24];
  repeated float mean_value = 6;
  optional bool add_parts = 7 [default = false];
  optional string parts_xml_dir = 8;
  optional bool add_kps = 9 [default = true];
  optional bool add_mask = 10 [default = true];
  repeated string xml_list_multiple = 11;
  repeated string xml_root_multiple = 12;
}
```



## Optical Flow来捕捉视频中的Motion信息

 https://www.zhihu.com/question/52185576 引入了时间序列, ，可以将预测的Feature Map和当前帧计算出来的Feature Map融合起来一起输出结果

论文 : Video Detection中的稳定性（Stability）的问题  https://arxiv.org/abs/1611.06467  

研究的核心多集中于后者，选什么样的特征表示来描述你锁定的区域（HOG, C-SIFT, Haar, LBP, CNN, Deformable Part Models (DPM)  and etc.），将这些特征输入到什么样的分类器（SVM，Adaboost and etc.）进行打分，判断是否是我们要找的目标。

 尽管我们要检测的目标可能外形变化多端（由于品种，形变，光照，角度等等），通过大量数据训练CNN得到的特征表示还是能很好地帮助实现识别和判定的过程。但是有些极端情况下，如目标特别小，或者目标和背景太相似，或者在这一帧图像中因为模糊或者其他原因，目标确实扭曲的不成样子，CNN也会觉得力不从心，认不出来它原来是我们要找的目标呢。另外一种情况是拍摄场景混入了其他和目标外观很像的东西 (比如飞机和展翅大鸟)，这时候也可能存在误判。

单帧不够，多帧来凑

**第一种：侧重于目标的运动信息。**先基于motion segmentation 或是 background extraction（光流法和高斯分布等）实现对前景和背景的分离，也就是说我们借助运动信息挑出了很有可能是目标的区域；再考虑连续帧里目标的持续性（大小，颜色，轨迹的一致性），可以帮助删去一部分不合格的候选的目标区域；然后对挑出的区域打分做判断，还是利用外观信息（单帧里提到的）。

**第二种：动静结合，即在第一种的基础上，加入目标的外观形变。**有些目标在视频中会呈现幅度较大的，有一定规律的形变，比如行人和鸟。这时我们可以通过学习形变规律，总结出目标特殊的运动特征和行为范式，然后看待检测的目标是否满足这样的行为变化。常见的行为特征表示有3D descriptors，Markov-based shape dynamics, pose/primtive action-based histogram等等。这种综合目标静态和动态信息来判断是否是特定目标的方法，有些偏向action classification。

1. **与图像目标检测的区别**

如Naiyan Wang 大佬所说，视频中多了时序上下文关系（Temporal Context）。充分利用好时序上下文关系，可以解决视频中连续帧之间的大量冗余的情况，提高检测速度；还可以提高检测质量，解决视频相对于图像存在的运动模糊、视频失焦、部分遮挡以及奇异姿势等问题。

一下是一些参考论文:

[deep feature flow for video recognition ](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1611.07715)

[flow- guided feature aggregation for video object detection ](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1703.10025)

[towards high performance video object detection ](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1711.11577)

[towards high performance video object detection for mobiles ](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1804.05830)

 

##  训练新层



卷基层 设置   

```protobuf
 param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }

```

反卷积

```protobuf
layer {
  name: "conv6_5_deconv"
  type: "Deconvolution"
  bottom: "conv6_5"
  top: "conv6_5_deconv"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 0
    kernel_size: 2
    group: 1
    stride: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0.0
    }
  }
}
```



 

## 16:9

模型测试,  程序熟悉, 	

对比   这几个模型之间的不同之处, baseline 是	

`/home/xjx/xjx_old/models/ResNetPoseDet_JointTrain_I_L_fixedprior/Proto/train_DetNOPARTSAndPose_ShareParam_multiresize_fixedprior_more_16.prototxt`



/home/xjx/xjx_old/models/ResNetPoseDet_JointTrain_I_L_fixedprior/Proto/train_DetNOPARTSAndPose_ShareParam_multiresize_fixedprior_more_16.prototxt

/home/xjx/xjx_old/models/ResNetPoseDet_JointTrain_I_L_fixedprior/Proto/train_DetNOPARTSAndPose_ShareParam_rfcnresize_fixedprior.prototxt

- 用了新层, BBox_data_rfcn, 没有之前的slice reshape 操作, loss 也只有4个

/home/xjx/xjx_old/models/ResNetPoseDet_JointTrain_I_L_fixedprior/Proto/train_I-L.prototxt
/home/xjx/xjx_old/models/ResNetPoseDet_JointTrain_I_L_fixedprior/Proto/train_DetNOPARTSAndPose_ShareParam_rfcnresize_rfcnprior.prototxt
/home/xjx/xjx_old/models/ResNetPoseDet_JointTrain_I_L_fixedprior/Proto/train_DetNOPARTSAndPose_ShareParam_multiresize_rfcnprior.prototxt
/home/xjx/xjx_old/models/ResNetPoseDet_JointTrain_I_L_fixedprior/Proto/train_DetNOPARTSAndPose_ShareParam_multiresize_one_fixedprior.prototxt
/home/xjx/xjx_old/models/ResNetPoseDet_JointTrain_I_L_fixedprior/Proto/train_DetNOPARTSAndPose_ShareParam_multiresize_fixedprior.prototxt
/home/xjx/xjx_old/models/ResNetPoseDet_JointTrain_I_L_fixedprior/Proto/train_DetNOPARTSAndPose_ShareParam_big_batchsize.prototxt

  

 把多尺度 移植到 新的模型上面, 

1. 这几个模型之间有什么区别
2. data层输出是什么东西
3. 需要调什么参数吗?? 
4. 需要修改哪些层??
5. 层之间连接正确吗 blob 的shape 正确吗
6. 训练和测试的时候需要修改哪些东西

 

加入多尺度的步骤:

1. name: "data"
     type: "BBoxData"    

   改为  name: "data_det"
     type: "BBoxDataMultiResize"

2. resized_width: 288  改为  resized_width: 512

3. batch_size 为8

 

### 感受野计算

​     **RF = 1 #待计算的feature map上的感受野大小**
　　**for layer in （top layer To down layer）:**
　　　　**RF = ((RF -1)\* stride) + fsize**

stride 表示卷积的步长； fsize表示卷积层滤波器的大小　　 

 

#  c++ 学习

1、rand()不需要参数，它会返回一个从0到最大随机数的任意整数，最大随机数的大小通常是固定的一个大整数。

2、如果你要产生0~99这100个整数中的一个随机整数，可以表达为：int num = rand() % 100; 

​     这样，num的值就是一个0~99中的一个随机数了。

3、如果要产生1~100，则是这样：int num = rand() % 100 + 1;  

4、总结来说，可以表示为：int num = rand() % n +a;

​     其中的a是起始值，n-1+a是终止值，n是整数的范围。

5、一般性：rand() % (b-a+1)+ a ;    就表示  a~b 之间的一个随机整数。

6、若要产生0~1之间的小数，则可以先取得0~10的整数，然后均除以10即可得到“随机到十分位”的10个随机小数。

​     若要得到“随机到百分位”的随机小数，则需要先得到0~100的10个整数，然后均除以100，其它情况依 此类推。

7、通常rand()产生的随机数在每次运行的时候都是与上一次相同的，这样是为了便于程序的调试。

​     若要产生每次不同的随机数，则可以使用srand( seed )函数进行产生随机化种子，随着seed的不同，就能够产生        不同的随机数。

8、还可以包含time.h头文件，然后使用srand(time(0))来使用当前时间使随机数发生器随机化，这样就可以保证每两        次运行时可以得到不同的随机数序列，同时这要求程序的两次运行的间隔超过1秒。 

 

### vector<cv::Mat>  note: 

方法一

```c++
std::vector<cv::Mat> template_img;
template_img.push_back(img);
```

方法二

```
std::vector<cv::Mat> template_img;
template_img.push_back(img.clone());
```

区别：方法二vector中的元素添加正确

分析：出现这样问题的原因，可以查看vector的push_back()函数及Mat的copy constructor

```c++
template <typename Dtype>
struct AnnoData {
  // 图片路径
  string img_path;
  // 数据集
  string dataset;
  // 图片尺寸
  int img_width;
  int img_height;
  // 实例数
  int num_person;
  // 实例数据结构集合 
  vector<Instance<Dtype> > instances;
};
		struct Instance {
          // minibatch内部的编号
          int bindex;
          // 类别号：0
          int cid;
          // 类下的实例号
          int pid;
          // 该对象是否是diff？
          bool is_diff;
          // 该对象是否是crowd?
          bool iscrowd;
          // 该对象的Box
          BoundingBox<Dtype> bbox;
          // 该对象是否有mask?
          bool mask_included;
          // 该对象的Mask图片的位置
          string mask_path;
          // 该对象是否有关节点?
          bool kps_included;
          // 该对象的可见关节点数量
          int num_kps;
          // 该对象的关节点数据
          Joints joint;
        };
			BoundingBox  
				  Dtype x1_, y1_, x2_, y2_;
              	   Dtype scale_factor_;
            };	
            Joints
            	  struct Joints {
                    vector<Point2f> joints;
                    vector<int> isVisible;
                  };
```

# 7-13

cvpr 新的文章 需要多看看

看数字图像处理 



遇到问题  16:9  , loss 变成nan,  查看了在113 训练的模型, 更改了很多地方,  1. solver中的 iter_size, 改成2,  type:sgd  改为 type:adam, 

​	

2. data  改为 BBoxDataMultiResize
3. 在 train,prototxt 中 修改了  resize_width  改为512,  resize_heigh 改为512  ,  所有的 lr_mult 改为 0.1  decay)mult weight的 改为0.1
4. featuremap1_1_mbox_priorbox 中的 type 改为 PriorBoxFixedSize
5. featuremap1_1_mbox_priorbox 的 pro_width 改为 0.x*512,  所有的数值乘以512 , 



总结一下怎么去看 这些东西: 

先看prototxt , 这里面有很多参数, 先根据参数  熟悉一下这个层有哪些东西可以调整,  明确输入输出是什么